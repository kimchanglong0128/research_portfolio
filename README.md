# ğŸ« CHANGLONG JIN

Hi, I'm **CHANGLONG JIN**, currently pursuing an Integrated Masterâ€“Ph.D. in Artificial Intelligence at **Yonsei University**.  
My research aims to develop **generative models with stronger inductive biases**, enabling them to maintain semantic consistency, preserve identity, and continuously self-improve through multimodal feedback.

I am particularly interested in:
- score-based generative modeling  
- multimodal alignment and consistency  
- self-refining generative systems  
- parameter-efficient adaptation (LoRA, Adapters)  


---

# ğŸ¯ Research Focus

- **Inductive Biases in Generative Models**  
- **Identity-Stable Diffusion SDE/ODE Dynamics**  
- **Closed-Loop Textâ€“Image â†” Imageâ€“Text Multimodal Alignment**  
- **Parameter-Efficient Personalization & Adaptation**  
- **Self-Correction in Generative Systems**  
- **Multimodal Fusion (Image Ã— Text)**  
- **Improving Semantic Faithfulness in T2I Models**  


---

# ğŸ”¬ Current Research Directions

## ğŸ§© Proposal 1 â€” Identity-Preserving Diffusion Dynamics
One-shot personalization often suffers from identity drift in diffusion models.  
I investigate **how identity information propagates through diffusion SDE/ODE fields**, and develop:

- low-rank and structure-aware inductive biases for identity preservation  
- identity-stable probability-flow ODE formulations  
- dynamics-level controls for stabilizing identity across timesteps  

**Goal:** A theoretically grounded, identity-stable personalization framework under minimal samples.


---

## ğŸ” Proposal 2 â€” Closed-Loop Multimodal Alignment
Current text-to-image models lack mechanisms for **self-evaluation and self-correction**.  
I propose a **closed-loop T2I â†” I2T alignment framework** using:

- CLIP- and caption-based multimodal rewards  
- iterative cycle consistency to reduce semantic drift  
- lightweight adapter-based continuous refinement  

**Goal:** A self-improving multimodal generative system with robust alignment, especially for long-tail or compositional prompts.


---

# ğŸ§­ Preliminary Work

## ğŸ“š Research Preparation
I conducted in-depth reviews and theoretical studies on:
- Generative & Diffusion Models  
- Multimodal Alignment and Representation Learning  
- Parameter-efficient Adaptation Methods  

(Research notes documented in personal logs.)

---

## ğŸ§ª Research Projects & Experiments
- LoRA-based diffusion personalization  
- Cycle-consistency experiments  
- Multimodal fusion (vision Ã— text)  
- Score-based model behavior analysis  


---

# ğŸš€ Research Roadmap
![Roadmap](images/RoadMap.png)


---

# ğŸ“« Contact

- Email: **kimcl1221@yonsei.ac.kr**  
- GitHub: **https://github.com/kimchanglong0128**
